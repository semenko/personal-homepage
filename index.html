<!--
  This comical header is inspired by Eric Mill's epic IsItChristmas.com.
  nick.semenkovich.com takes your security very seriously.
  The Semenkovich Family domain name "*.semenkovich.com", is hardcoded into modern
  browsers, and shipped in every update of Chrome and Firefox, as part of the HSTS Preload List.
  If you notice any security issues, email nick@semenkovich.com right away!
-->
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <meta name="author" content="Nick Semenkovich (semenko)" />
  <meta name=viewport content="width=800">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }

    .circlecrop{
      width: 180px;

         position: relative;
         overflow: hidden;
         border-radius: 50%;
    }

    #footer, #footer a {
      font-size: 10px;
    }
  </style>
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <title>Nick Semenkovich (semenko)</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-45476333-1', {siteSpeedSampleRate:100});
    ga('send', 'pageview');
  </script>
</head>
<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Nick Semenkovich</name>
              </p>



              <p>I graduated from MIT in 2009 with a B.S. in Course 6
                (EECS) and a B.S. in Course 7 (Biology). I also served
                as Editor in Chief of the student
                newspaper The Tech, and now I'm frequently on PBS's Stay Tuned as a <a href="http://staytuned.ninenet.org/contributors/nick-semenkovich/">science & health contributor</a>.</p>
              <p>I'm currently pursuing an M.D./Ph.D. in
            the <a href="http://gordonlab.wustl.edu">Gordon Lab</a> at  <a href="http://mstp.wustl.edu">Washington
            University School of Medicine</a> as part of the
                NIH <a href="https://www.nigms.nih.gov/Training/InstPredoc/PredocOverview-MSTP.htm">Medical
            Scientist Training Program</a>.</p>
              <p>When I'm not in lab, I'm either studying web app security
              (I've been recognized by Google, <a href="http://www.securityfocus.com/bid/58333/info">among</a> <a href="http://blog.maxcdn.com/bootstrapcdn-security-post-mortem/">others</a>), or contributing to open-source projects
                such as the EFF's plugin <a href="https://www.eff.org/https-everywhere">HTTPS Everywhere</a>.</p>
              <p>In the medical realm, I sometimes hack on advancing <a href="http://www.whitecoatacademy.org/">open educational
              resources</a> (which lag far behind even MIT's OpenCourseWare project).
              I'm also hoping to ensure <a href="http://www.vaccinatedoctors.org/">doctors &amp; nurses
              get flu vaccines</a>, which (strangely) aren't required for health care
              workers &mdash; even though many other vaccines are (my Missouri representative recently introduced <a href="http://www.senate.mo.gov/17info/BTS_Web/Bill.aspx?SessionType=R&BillID=57095289">a bill</a>).</p>
              <p>If you're incredibly bored, you can follow
              my <a href="https://minorplanetcenter.net/db_search/show_object?object_id=18015">main-belt
              asteroid</a>, which looks like it's having a great time just a bit past Mars.</p>
              <p>
                I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>. I've spent time at <a href="https://en.wikipedia.org/wiki/Google_X">Google[x]</a>, <a href="http://groups.csail.mit.edu/vision/welcome/">MIT CSAIL</a>, <a href="http://www.captricity.com/">Captricity</a>, <a href="https://www.nasa.gov/ames">NASA Ames</a>, <a href="http://www.google.com/">Google NYC</a>, the <a href="http://mrl.nyu.edu/">NYU MRL</a>, <a href="http://www.nibr.com/">Novartis</a>, and <a href="http://www.astrometry.net/">Astrometry.net</a>. I did my bachelors at the <a href="http://cs.toronto.edu">University of Toronto</a>.
              </p>
              <p align=center>
                <a href="mailto:semenko@alum.mit.edu">Email</a> &nbsp/&nbsp
                <a href="https://twitter.com/semenko/">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/semenko/">GitHub</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/semenko/"> LinkedIn </a>
              </p>
            </td>
            <td width="33%">
              <div class="circlecrop">
               <img src="image.png">
            </div>
            </td>
          </tr>
        </table>
        <!--
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, machine learning, statistics, optimization, image processing, virtual reality, and computational photography. Much of my research is about inferring the physical world (shape, depth, motion, paint, light, colors, etc) from images. I have also worked in astronomy and biology. Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

          <tr>
            <td width="25%">
              <div class="one">
                <img src='images/darkflash_before.png'>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/1901.01370">
                <papertitle>Stereoscopic Dark Flash for Low-light Photography</papertitle>
              </a>
              <br>
              <a href="https://www.andrew.cmu.edu/user/jianwan2/">Jian Wang</a>,
              <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,
              <strong>Jonathan T. Barron</strong>
              <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
              <br>
              <em>arXiv Preprint</em>, 2019
              <br>
              <p></p>
              <p>
              	By making one camera in a stereo pair hyperspectral we can multiplex dark flash pairs in space instead of time.
              </p>
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                <div class="two" id='unprocessing_image'><img src='images/unprocessing_after.jpg'></div>
                <img src='images/unprocessing_before.jpg'>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/1811.11127">
                <papertitle>Unprocessing Images for Learned Raw Denoising</papertitle>
              </a>
              <br>
              <a href="http://timothybrooks.com/">Tim Brooks</a>,
              <a href="https://people.eecs.berkeley.edu/~bmild/">Ben Mildenhall</a>,
              <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,
              <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
              <a href="http://www.dsharlet.com/">Dillon Sharlet</a>,
              <strong>Jonathan T. Barron</strong>
              <br>
              <em>arXiv Preprint</em>, 2018
              <br>
              <a href="http://timothybrooks.com/tech/unprocessing/">project page</a>
              <p></p>
              <p>We can learn a better denoising model by processing and unprocessing images the same way a camera does.</p>
            </td>
          </tr>

		  <tr>
            <td width="25%">
              <div class="one">
                <img src='images/motionblur_before.jpg'>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/1811.11745">
                <papertitle>Learning to Synthesize Motion Blur</papertitle>
              </a>
              <br>
              <a href="http://timothybrooks.com/">Tim Brooks</a>,
              <strong>Jonathan T. Barron</strong>
              <br>
              <em>arXiv Preprint</em>, 2018
              <br>
              <a href="http://timothybrooks.com/tech/motion-blur/">project page</a> /
              <a href="https://www.youtube.com/watch?v=8T1jjSz-2V8">video</a>
              <p></p>
              <p>Frame interpolation techniques can be used to train a network to directly synthesize linear motion blur.</p>
            </td>
          </tr>

		  <tr bgcolor="#ffffd0">
            <td width="25%">
              <div class="one">
                <img src='images/loss_before.png'>
              </div>
            </td>
            <td valign="top" width="75%">
              <p>
                <a href="https://arxiv.org/abs/1701.03077">
                  <papertitle>A General and Adaptive Robust Loss Function</papertitle>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>
                <br>
                <em>arXiv Preprint</em>, 2018
                <br>
                <p></p>
                <p>A single robust loss function is a superset of many other common robust loss functions,
                   and allows training to automatically adapt the robustness of its own loss.</p>
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                <img src='images/motionstereo_before.png'>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://drive.google.com/file/d/1AABFJ3NgD5DAo5JEpEjWZrcQNzjZnvW9/view?usp=sharing">
              <papertitle>Depth from Motion for Smartphone AR</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/valentinjulien/">Julien Valentin</a>,
              <a href="https://www.linkedin.com/in/adarshkowdle/">Adarsh Kowdle</a>,
              <strong>Jonathan T. Barron</strong>, <a href="http://people.csail.mit.edu/nwadhwa/">Neal Wadhwa</a>,
              and others
              <br>
              <em>SIGGRAPH Asia</em>, 2018
              <br>
              <a href="data/Valentin2018.bib">bibtex</a>
              <p></p>
              <p>Depth cues from camera motion allow for real-time occlusion effects in augmented reality applications.</p>
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                <img src='images/portrait_before.jpg'>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://drive.google.com/file/d/13i6DlS9UhGVKmwslLUFnKBwdxFRVQeQj/view?usp=sharing">
                <papertitle>Synthetic Depth-of-Field with a Single-Camera Mobile Phone</papertitle>
              </a>
              <br>
              <a href="http://people.csail.mit.edu/nwadhwa/">Neal Wadhwa</a>,
              <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>,
              <a href="http://graphics.stanford.edu/~dejacobs/">David E. Jacobs</a>, Bryan E. Feldman, Nori Kanazawa, Robert Carroll,
              <a href="http://www.cs.cmu.edu/~ymovshov/">Yair Movshovitz-Attias</a>,
              <strong>Jonathan T. Barron</strong>, Yael Pritch,
              <a href="http://graphics.stanford.edu/~levoy/">Marc Levoy</a>
              <br>
              <em>SIGGRAPH</em>, 2018
              <br>
              <a href="https://arxiv.org/abs/1806.04171">arxiv</a> /
              <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">blog post</a> /
              <a href="data/Wadhwa2018.bib">bibtex</a>
              <p></p>
              <p>Dual pixel cameras and semantic segmentation algorithms can be used for shallow depth of field effects.</p>
              <p>This system is the basis for "Portrait Mode" on the Google Pixel 2 smartphones</p>
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                <img src='images/aperture_before.jpg'>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://drive.google.com/open?id=1MpvxcW7OTJP321QL_q4ZLQ8D653bZZzy">
                <papertitle>Aperture Supervision for Monocular Depth Estimation</papertitle>
              </a>
              <br>
              <a href="https://people.eecs.berkeley.edu/~pratul/">Pratul P. Srinivasan</a>,
              <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>,
              <a href="http://people.csail.mit.edu/nwadhwa/">Neal Wadhwa</a>,
              <a href="http://graphics.stanford.edu/~renng/">Ren Ng</a>,
              <strong>Jonathan T. Barron</strong>
              <br>
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2018
              <br>
              <a href="https://github.com/google/aperture_supervision">code</a> /
              <a href="data/Srinivasan2018.bib">bibtex</a>
              <p></p>
              <p>Varying a camera's aperture provides a supervisory signal that can teach a neural network to do monocular depth estimation.</p>
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                <img src='images/deepburst_before.png'>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://drive.google.com/file/d/1GAH8ijyZ7GnoBnQFANEzdXinHrE4vvXn/view?usp=sharing">
                <papertitle>Burst Denoising with Kernel Prediction Networks</papertitle>
              </a>
              <br>
              <a href="https://people.eecs.berkeley.edu/~bmild/">Ben Mildenhall</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
              <a href="http://www.dsharlet.com/">Dillon Sharlet</a>,
              <a href="http://graphics.stanford.edu/~renng/">Ren Ng</a>, Robert Carroll
              <br>
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2018 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font>
              <br>
              <a href="https://drive.google.com/file/d/1aqk3Q-L2spjLZh2yRWKUWIDcZkGjQ7US/view?usp=sharing">supplement</a> /
              <a href="https://github.com/google/burst-denoising">code</a> /
              <a href="data/Mildenhall2018.bib">bibtex</a>
              <p></p>
              <p>We train a network to predict linear kernels that denoise noisy bursts from cellphone cameras.</p>
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                <img src='images/friendly_before.png'>
              </div>
            </td>
            <td valign="top" width="75%">
              <p>
                <a href="https://drive.google.com/file/d/1w_0djhL0QgC_fbehnJ0c-J23_kW_420p/view?usp=sharing">
                  <papertitle>A Hardware-Friendly Bilateral Solver for Real-Time Virtual Reality Video</papertitle>
                </a>
                <br>
                <a href="https://homes.cs.washington.edu/~amrita/">Amrita Mazumdar</a>, <a href="http://homes.cs.washington.edu/~armin/">Armin Alaghi</a>, <strong>Jonathan T. Barron</strong>, <a href="https://www.cs.unc.edu/~gallup/">David Gallup</a>, <a href="https://homes.cs.washington.edu/~luisceze/">Luis Ceze</a>, <a href="https://homes.cs.washington.edu/~oskin/">Mark Oskin</a>, <a href="http://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a>
                <br>
                <em>High-Performance Graphics (HPG)</em>, 2017
                <br>
                <a href="https://sampa.cs.washington.edu/projects/vr-hw.html">project page</a>
                <p></p>
                <p>A reformulation of the bilateral solver can be implemented efficiently on GPUs and FPGAs.</p>
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                <img src='images/hdrnet_before.jpg'>
              </div>
            </td>
            <td valign="top" width="75%">
              <p>
                <a href="https://drive.google.com/file/d/1jQY3CTMnLX7PeGUzYLso9H1eCsZyWbwg/view?usp=sharing">
                  <papertitle>Deep Bilateral Learning for Real-Time Image Enhancement</papertitle>
                </a>
                <br>
                <a href="http://www.mgharbi.com">Micha&euml;l Gharbi</a>, <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>, <strong>Jonathan T. Barron</strong>, <a href="https://people.csail.mit.edu/hasinoff/">Samuel W. Hasinoff</a>, <a href="http://people.csail.mit.edu/fredo/">Fr&eacute;do Durand </a>
                <br>
                <em>SIGGRAPH</em>, 2017
                <br>
                <a href="https://groups.csail.mit.edu/graphics/hdrnet/">project page</a> /
                <a href="https://www.youtube.com/watch?v=GAe0qKKQY_I">video</a> /
                <a href="data/GharbiSIGGRAPH2017.bib">bibtex</a> /
                <a href="http://news.mit.edu/2017/automatic-image-retouching-phone-0802">p</a><a href="https://www.wired.com/story/googles-new-algorithm-perfects-photos-before-you-even-take-them/">r</a><a href="https://petapixel.com/2017/08/02/new-ai-can-retouch-photos-snap/">e</a><a href="https://www.theverge.com/2017/8/2/16082272/google-mit-retouch-photos-machine-learning">s</a><a href="http://gizmodo.com/clever-camera-app-uses-deep-learning-to-perfectly-retou-1797474282">s</a>
                <p></p>
                <p>By training a deep network in bilateral space we can learn a model for high-resolution and real-time image enhancement.</p>
            </td>
          </tr>

          <tr onmouseout="ffcc_stop()" onmouseover="ffcc_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='ffcc_image'><img src='images/ffcc_after.jpg'></div>
                <img src='images/ffcc_before.jpg'>
              </div>
              <script type="text/javascript">
                function ffcc_start() {
                  document.getElementById('ffcc_image').style.opacity = "1";
                }

                function ffcc_stop() {
                  document.getElementById('ffcc_image').style.opacity = "0";
                }
                ffcc_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <p>
                <a href="https://drive.google.com/file/d/1VDWAS7HgiufTNPP7CQY00KmJP71QIZAy/view?usp=sharing">
                  <papertitle>Fast Fourier Color Constancy</papertitle>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>, Yun-Ta Tsai
                <br>
                <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2017
                <br>
                <a href="https://drive.google.com/file/d/1b5zdR5UYPTkXa2UgiLhi-PP89bzINJSR/view?usp=sharing">supplement</a> /
                <a href="https://youtu.be/rZCXSfl13rY">video</a> /
                <a href="data/BarronTsaiCVPR2017.bib">bibtex</a> /
                <a href="https://github.com/google/ffcc">code</a> /
                <a href="https://drive.google.com/open?id=0B4nuwEMaEsnmWkJQMlFPSFNzbEk">output</a> /
                <a href="https://blog.google/products/photos/six-tips-make-your-photos-pop/">blog post</a> /
                <a href="https://9to5google.com/2017/03/03/google-photos-auto-white-balance/">p</a><a href="https://www.engadget.com/2017/03/03/google-photos-automatically-fixes-your-pictures-white-balance/">r</a><a href="https://lifehacker.com/google-photos-will-now-automatically-adjust-the-white-b-1793009155">e</a><a href="https://petapixel.com/2017/03/06/google-photos-will-now-automatically-white-balance-snapshots/">s</a><a href="http://www.theverge.com/2017/3/3/14800062/google-photos-auto-white-balance-android">s</a>
                <p></p>
                <p>Color space can be aliased, allowing white balance models to be learned and evaluated in the frequency domain. This improves accuracy by 13-20% and speed by 250-3000x.</p>
                <p>This technology is used by <a href="https://photos.google.com/">Google Photos</a> and <a href="https://www.google.com/maps">Google Maps</a>.</p>
            </td>
          </tr>

          <tr onmouseout="jump_stop()" onmouseover="jump_start()" bgcolor="#ffffd0">
            <td width="25%">
              <div class="one">
                <div class="two" id='jump_image'><img src='images/jump_anim.gif'></div>
                <img src='images/jump_still.png'>
              </div>
              <script type="text/javascript">
                function jump_start() {
                  document.getElementById('jump_image').style.opacity = "1";
                }

                function jump_stop() {
                  document.getElementById('jump_image').style.opacity = "0";
                }
                jump_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <p>
                <a href="https://drive.google.com/file/d/1RBnTrtzqmuO8uj3GQaR5vBJZjIC3Jxjn/view?usp=sharing">
                  <papertitle>Jump: Virtual Reality Video</papertitle>
                </a>
                <br>
                <a href="http://mi.eng.cam.ac.uk/~ra312/">Robert Anderson</a>, <a href="https://www.cs.unc.edu/~gallup/">David Gallup</a>, <strong>Jonathan T. Barron</strong>, <a href="https://mediatech.aalto.fi/~janne/index.php">Janne Kontkanen</a>, <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>, <a href="http://carlos-hernandez.org/">Carlos Hern&aacutendez</a>, <a href="https://homes.cs.washington.edu/~sagarwal/">Sameer Agarwal</a>, <a href="https://homes.cs.washington.edu/~seitz/">Steven M Seitz</a>
                <br>
                <em>SIGGRAPH Asia</em>, 2016
                <br>
                <a href="https://drive.google.com/file/d/11D4eCDXqqFTtZT0WS2COJE0hsAN3QEww/view?usp=sharing">supplement</a> /
                <a href="https://www.youtube.com/watch?v=O0qUYynupTI">video</a> /
                <a href="data/Anderson2016.bib">bibtex</a> /
                <a href="https://blog.google/products/google-vr/jump-using-omnidirectional-stereo-vr-video/">blog post</a>
                <p></p>
                <p>Using computer vision and a ring of cameras, we can make video for virtual reality headsets that is both stereo and 360&deg;.</p>
                <p>This technology is used by <a href="https://vr.google.com/jump/">Jump</a>. </p>
                <p></p>
                </a>
              </p>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Projects</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/prl.jpg" alt="prl" width="160" height="160"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://drive.google.com/file/d/13rVuJpcytRdLYCnKpq46g7B7IzSrPQ2P/view?usp=sharing">
                  <papertitle>Parallelizing Reinforcement Learning</papertitle>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://www.eecs.berkeley.edu/~dsg/">Dave Golland</a>, <a href="http://www.cs.berkeley.edu/~nickjhay/">Nicholas J. Hay</a>, 2009
                <p>
                  <br> Markov Decision Problems which lie in a low-dimensional latent space can be decomposed, allowing modified RL algorithms to run orders of magnitude faster in parallel.
                </p>
              </p>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/pacman.jpg" alt="pacman" width="160" height="160"></td>
            <td width="75%" valign="center">
              <p>
                <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">
                  <papertitle>CS188 - Fall 2010 (GSI)</papertitle>
                </a>
                <br>
                <br>
                <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">
                  <papertitle>CS188 - Spring 2011 (GSI)</papertitle>
                </a>
                <br>
              </p>
            </td>
          </tr>
        </table>
        //-->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <br>
              <p align="right" id="footer">
                  Inspired by <a href="https://github.com/jonbarron/jonbarron_website">this website</a> &amp; <a href="https://github.com/semenko/personal-homepage">open source at GitHub</a>.
              </p>
            </td>
          </tr>
        </table>
        </td>
    </tr>
  </table>
</body>

</html>
